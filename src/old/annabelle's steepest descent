//////////////////////////////////////////////////Training Code (Gradient Descent)//////////////////////////////////////////////////
    /**
     * Finds the total error of the function and updates the previous error.
     */
    double findTotalError()
    {
        double error = 0.0;
        for (int i = 0; i < outputNodes; i++)
        {
            error += 0.5 * ((expectedOutputs[i] - activations[i][activations[0].length - 1]) *
                  (expectedOutputs[i] - activations[i][activations[0].length - 1]));
        }
        error = Math.sqrt(error);
        return error;
    }


    /**
     * Uses the gradient descent to train and optimize each weight for the given problem.
     * Modifies the weights using an adaptive lambda, increasing lambda if error decreases and decreasing otherwise.
     * <p>
     * Termination conditions:
     * 1. Total Error is less than or equal to a predetermined threshold.
     * 2. Lambda = 0.
     * 3. Change in Error = 0.
     * 4. The number of iterations goes above another predetermined threshold.
     */
    public void steepestDescent()
    {
        final int LAMBDA_CHANGE_CONSTANT = 2;   // The amount by which lambda will be multiplied or divided depending on the error.

        double prevError = findTotalError();
        double newError = prevError;
        double output = activations[numTrials - 1][0]; //alter this when more than one output
        int iteration = 0;
        double[][][] prevWeights = new double[numberActivationLayers - 1][maxNumberNodes][maxNumberNodes];

        //copy over weight values to prevWeights: make this a separate method later
        for (int m = 0; m < prevWeights.length; m++)
        {
            for (int source = 0; source < maxNumberNodes; source++)
            {
                for (int dest = 0; dest < maxNumberNodes; dest++)
                {
                    prevWeights[m][source][dest] = weights[m][source][dest];
                }
            }
        }

        while (iteration == 0 || (newError > errorThreshold && learningFactor != 0.0 && Math.abs(newError - prevError) > changeThreshold &&
              iteration < iterationThreshold))
        {
            //recalculate all weights: for only one output + one hidden layer (fix when more layers/outputs)
            //recalculate for m=1 weights (second weight layer):
            for (int j = 0; j < activations[numLayers - 2].length; j++)
            {
                double deltaW = (-learningFactor) * (-(trainingValue - output) *
                      (derivActivationFunction(propagationFunction(numLayers - 1, 0))) * activations[numLayers - 2][j]);

                weights[numLayers - 2][j][0] += deltaW;
            }

            //recalculate for m=0 weights (first weight layer):
            for (int k = 0; k < activations[0].length; k++)
            {
                for (int j = 0; j < activations[numLayers - 2].length; j++)
                {
                    double deltaW = (-activations[0][k]) * (derivActivationFunction(propagationFunction(numLayers - 2, j)) *
                          (trainingValue - output) * derivActivationFunction(propagationFunction(numLayers - 1, 0)) *
                          weights[numLayers - 2][j][0]);

                    weights[0][k][j] += deltaW;
                }
            }

            //recalculate output with new weights
            findOutput();
            output = activations[numLayers - 1][0];

            //set new error values
            if (iteration > 0)
            {
                prevError = newError;
            }
            newError = errorCalculation();

            //check how newError compares to prevError and adjust learningFactor accordingly:
            if (newError < prevError)
            {
                learningFactor *= lambdaChange;

                //set prevWeights to store new weight values for next cycle
                for (int m = 0; m < prevWeights.length; m++)
                {
                    for (int source = 0; source < numRows; source++)
                    {
                        for (int dest = 0; dest < numRows; dest++)
                        {
                            prevWeights[m][source][dest] = weights[m][source][dest];
                        }
                    }
                }
            }
            else
            {
                learningFactor /= lambdaChange;

                //reset weights to before this cycle
                for (int m = 0; m < prevWeights.length; m++)
                {
                    for (int source = 0; source < numRows; source++)
                    {
                        for (int dest = 0; dest < numRows; dest++)
                        {
                            weights[m][source][dest] = prevWeights[m][source][dest];
                        }
                    }
                }
            }

            iteration++;
            System.out.println("Learning Factor: " + learningFactor);
            System.out.println("Current Error: " + newError + "\n");

        } //while (newError>errorThreshold && learningFactor!=0 && newError-prevError>changeThreshold &&
        //        iteration<iterationThreshold)

        System.out.println("Number of Iterations: " + iteration);
    }
    */
}